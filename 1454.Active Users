Table: Accounts

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
id is the primary key (column with unique values) for this table.
This table contains the account id and the user name of each account.
 

Table: Logins

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| login_date    | date    |
+---------------+---------+
This table may contain duplicate rows.
This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day.
 

Active users are those who logged in to their accounts for five or more consecutive days.

Write a solution to find the id and the name of active users.

Return the result table ordered by id.

The result format is in the following example.

 

Example 1:

Input: 
Accounts table:
+----+----------+
| id | name     |
+----+----------+
| 1  | Winston  |
| 7  | Jonathan |
+----+----------+
Logins table:
+----+------------+
| id | login_date |
+----+------------+
| 7  | 2020-05-30 |
| 1  | 2020-05-30 |
| 7  | 2020-05-31 |
| 7  | 2020-06-01 |
| 7  | 2020-06-02 |
| 7  | 2020-06-02 |
| 7  | 2020-06-03 |
| 1  | 2020-06-07 |
| 7  | 2020-06-10 |
+----+------------+
Output: 
+----+----------+
| id | name     |
+----+----------+
| 7  | Jonathan |
+----+----------+
Explanation: 
User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user.
User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user.




_____________________________________________________________________________________________________________________________________

Solution


from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AccountsLoginsExample").getOrCreate()

# ---------------------------
# Accounts table
accounts_data = [
    (1, "Winston"),
    (7, "Jonathan")
]
accounts_columns = ["id", "name"]

accounts_df = spark.createDataFrame(accounts_data, accounts_columns)

# ---------------------------
# Logins table
logins_data = [
    (7, "2020-05-30"),
    (1, "2020-05-30"),
    (7, "2020-05-31"),
    (7, "2020-06-01"),
    (7, "2020-06-02"),
    (7, "2020-06-02"),
    (7, "2020-06-03"),
    (1, "2020-06-07"),
    (7, "2020-06-10")
]
logins_columns = ["id", "login_date"]

logins_df = spark.createDataFrame(logins_data, logins_columns)

from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number,date_sub,count
logins_df.select(("*")).distinct()\
    .withColumn("rn", row_number().over(Window.partitionBy(col("id")).orderBy(col("login_date")) )   )\
    .withColumn("new_col", date_sub( col("login_date"), col("rn") )   )\
    .groupBy( col("id"), col("new_col")  )\
    .agg(count("*").alias("cnt"))\
    .filter(col("cnt") >= 5)\
    .select(col("id").alias("joined_id")   )\
    .join(accounts_df.alias("a"), on = col("a.id")==col("joined_id"), how = "inner" )\
    .select(col("joined_id"), col("a.name"))\
    .orderBy(col("joined_id")).show()
